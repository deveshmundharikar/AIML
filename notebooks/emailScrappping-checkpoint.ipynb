{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa0043c-06ff-4849-af6b-08acbda59dc3",
   "metadata": {},
   "source": [
    "start_urls list me jitni marzi websites daal do\n",
    "\n",
    "Har domain ko crawl karega (sirf apne domain ke andar hi links follow karega)\n",
    "\n",
    "Har jagah se unique emails nikal lega\n",
    "\n",
    "Sabhi emails ek hi multi_domain_emails.csv me save ho jaayenge\n",
    "\n",
    "Limit set karke targeted extraction\n",
    "Tum abhi 200 email limit use kar rahe ho, but agar sirf 50 chahiye to email_limit = 50 kar do.\n",
    "\n",
    "Crawl depth control\n",
    "Agar website bahut badi hai to depth_limit = 2 ya 1 rakhna best hoga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256f14be-6e0e-4ce8-bd9c-b525f18fc747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extraction complete! Collected 4 emails (max 200) and saved to limited_emails.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import csv\n",
    "\n",
    "# Multiple target websites\n",
    "start_urls = [\n",
    "    \"https://webscraper.io/test-sites\"\n",
    "]    # ithe list dayaychi ahe url jikdun email extraction hou shakte \n",
    "\n",
    "visited = set()\n",
    "emails = set()\n",
    "max_workers = 10   # parallel workers\n",
    "email_limit = 200  # stop after this many emails\n",
    "depth_limit = 2    # how deep to crawl\n",
    "\n",
    "def crawl_page(url, domain, depth):\n",
    "    if url in visited or len(emails) >= email_limit or depth > depth_limit:\n",
    "        return []\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=3)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    # Emails extract karo\n",
    "    found = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", response.text)\n",
    "    emails.update(found)\n",
    "    if len(emails) >= email_limit:\n",
    "        return []\n",
    "\n",
    "    # Links extract karo\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(url, a[\"href\"])\n",
    "        if urlparse(link).netloc == domain and link not in visited:\n",
    "            links.append((link, depth + 1))\n",
    "    return links\n",
    "\n",
    "def crawl_domain(start_url):\n",
    "    domain = urlparse(start_url).netloc\n",
    "    to_visit = [(start_url, 0)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        while to_visit and len(emails) < email_limit:\n",
    "            futures = [executor.submit(crawl_page, url, domain, depth) for url, depth in to_visit]\n",
    "            to_visit = []\n",
    "            for f in futures:\n",
    "                result = f.result()\n",
    "                if result:\n",
    "                    to_visit.extend(result)\n",
    "\n",
    "# Run for all domains\n",
    "for url in start_urls:\n",
    "    crawl_domain(url)\n",
    "    if len(emails) >= email_limit:\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"limited_emails.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Email\"])\n",
    "    for email in sorted(list(emails))[:email_limit]:\n",
    "        writer.writerow([email])\n",
    "\n",
    "print(f\"✅ Extraction complete! Collected {len(emails)} emails (max {email_limit}) and saved to limited_emails.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31432041-4718-4adf-96d7-cc71bdb890d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
